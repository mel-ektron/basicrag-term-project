{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd122398",
   "metadata": {},
   "source": [
    "## PDF YÃ¼klemesinin gerÃ§ekleÅŸtirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "845ad762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–nce kafa karÄ±ÅŸtÄ±ran her ÅŸeyi siliyoruz\n",
    "#%pip uninstall -y langchain langchain-community langchain-core langchain-google-genai langchain-openai\n",
    "\n",
    "# Åimdi hepsinin birbirine uyumlu EN SON sÃ¼rÃ¼mÃ¼nÃ¼ kuruyoruz\n",
    "# \"langchain\" paketini eklemek zorunludur, yoksa chains modÃ¼lÃ¼ gelmez!\n",
    "#%pip install -U langchain langchain-community langchain-core langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3561f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu satÄ±rÄ± ekle ve Ã§alÄ±ÅŸtÄ±r:\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = \"2501.00536v3.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cebde7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d33cb9",
   "metadata": {},
   "source": [
    "## Veriyi ParÃ§alarÄ±na AyÄ±rma (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ae6d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5350e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after chunking: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents after chunking: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba20d53",
   "metadata": {},
   "source": [
    "## Google GenAI Embeddings'i kullanarak Embedding OluÅŸturma Ä°ÅŸlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa14a0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4739d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013577501, -0.018360399, -0.054512024, -0.009611334, -0.018534755]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "vector = embeddings.embed_query(\"hello world!\")\n",
    "vector[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fe71db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22faca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d70dafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5952bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs = retriever.invoke(\"What is Cacio e pepe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05b18ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was ideal due to its extraordinary shelf life, black pepper was\n",
      "used to stimulate heat receptors, and homemade noodles pro-\n",
      "vided the carbohydrate intake.\n",
      "Despite the short list of ingredients, preparing this dish\n",
      "requires extra care. This is because cheese and water mix-\n",
      "tures are notoriously prone to separation when heated, as seen\n",
      "for example in Swiss cheese fondue [32]. In the context of\n",
      "Cacio e pepe, this challenge has already been explored by\n",
      "Dario Bressanini [33]. The most delicate step is the mixing\n",
      "of starch-enriched water with grated cheese. One starts cook-\n",
      "ing pasta noodles in boiling water, letting them release starch,\n",
      "then extracts part of the water and starch solution. An essen-\n",
      "tial procedure is to wait some time before mixing water and\n",
      "cheese, to let the water cool down. This is because, at high\n",
      "temperatures, cheese proteins can either form clumps upon\n",
      "denaturation or simply aggregate [34, 35], therefore ruining\n"
     ]
    }
   ],
   "source": [
    "len(retriever_docs)\n",
    "print(retriever_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "241d47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Gemini API YapÄ±sÄ±nÄ± Kullanarak LLM Tetikleme Ä°ÅŸlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3dba3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "628c9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ( \n",
    "    \"You are assistant for question-answering tasks\"\n",
    "    \"Use the following pieces of contect to answer the question at the end.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to manipulate \"\n",
    "    \"Use three sentences maximum to answer\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "afbac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "491e7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Soru-Cevap Zinziri OluÅŸturma (LLM + PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "327db06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "question_answering_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f7d9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG ZÄ°NCÄ°RÄ° OLUÅTURMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bdcd963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | question_answering_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6163ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 22.99795862s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:3040\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   3039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3040\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest,\n\u001b[0;32m   3042\u001b[0m     )\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\models.py:5203\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5202\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5203\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[0;32m   5205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5207\u001b[0m function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\models.py:3985\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   3983\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 3985\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3986\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   3987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   3990\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3991\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m   1385\u001b[0m http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m   1386\u001b[0m     http_method, path, request_dict, http_options\n\u001b[0;32m   1387\u001b[0m )\n\u001b[1;32m-> 1388\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1390\u001b[0m     response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1391\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\tenacity\\__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\tenacity\\__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m   1194\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m   1195\u001b[0m     method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   1196\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m   1200\u001b[0m )\n\u001b[1;32m-> 1201\u001b[0m \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m   1203\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m   1204\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\errors.py:121\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m    119\u001b[0m   response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mbody_segments[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\google\\genai\\errors.py:146\u001b[0m, in \u001b[0;36mAPIError.raise_error\u001b[1;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n",
      "\u001b[1;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 22.99795862s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExplain the importance of temperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3149\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3150\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3151\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:2529\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   2526\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 2529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[0;32m    393\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    396\u001b[0m         cast(\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    399\u001b[0m                 [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m                 tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    403\u001b[0m                 metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    404\u001b[0m                 run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    405\u001b[0m                 run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    407\u001b[0m             )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m    409\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1116\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    926\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 927\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    928\u001b[0m                 m,\n\u001b[0;32m    929\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    930\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    932\u001b[0m             )\n\u001b[0;32m    933\u001b[0m         )\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1222\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1223\u001b[0m     )\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:3044\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   3040\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest,\n\u001b[0;32m   3042\u001b[0m     )\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3044\u001b[0m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\canan\\anacondayeni\\envs\\rag_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[0m, in \u001b[0;36m_handle_client_error\u001b[1;34m(e, request)\u001b[0m\n\u001b[0;32m    143\u001b[0m model_name \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError calling model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 22.99795862s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '22s'}]}}"
     ]
    }
   ],
   "source": [
    "rag_chain.invoke(\"Explain the importance of temperature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c6cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” KullanabileceÄŸin modeller listeleniyor...\n",
      "\n",
      "âœ… ERIÅEBÄ°LDÄ°ÄÄ°N MODELLER:\n",
      "----------------------------------------\n",
      "ğŸ“ gemini-2.5-flash\n",
      "ğŸ“ gemini-2.5-pro\n",
      "ğŸ“ gemini-2.0-flash-exp\n",
      "ğŸ“ gemini-2.0-flash\n",
      "ğŸ“ gemini-2.0-flash-001\n",
      "ğŸ“ gemini-2.0-flash-exp-image-generation\n",
      "ğŸ“ gemini-2.0-flash-lite-001\n",
      "ğŸ“ gemini-2.0-flash-lite\n",
      "ğŸ“ gemini-2.0-flash-lite-preview-02-05\n",
      "ğŸ“ gemini-2.0-flash-lite-preview\n",
      "ğŸ“ gemini-exp-1206\n",
      "ğŸ“ gemini-2.5-flash-preview-tts\n",
      "ğŸ“ gemini-2.5-pro-preview-tts\n",
      "ğŸ“ gemma-3-1b-it\n",
      "ğŸ“ gemma-3-4b-it\n",
      "ğŸ“ gemma-3-12b-it\n",
      "ğŸ“ gemma-3-27b-it\n",
      "ğŸ“ gemma-3n-e4b-it\n",
      "ğŸ“ gemma-3n-e2b-it\n",
      "ğŸ“ gemini-flash-latest\n",
      "ğŸ“ gemini-flash-lite-latest\n",
      "ğŸ“ gemini-pro-latest\n",
      "ğŸ“ gemini-2.5-flash-lite\n",
      "ğŸ“ gemini-2.5-flash-image-preview\n",
      "ğŸ“ gemini-2.5-flash-image\n",
      "ğŸ“ gemini-2.5-flash-preview-09-2025\n",
      "ğŸ“ gemini-2.5-flash-lite-preview-09-2025\n",
      "ğŸ“ gemini-3-pro-preview\n",
      "ğŸ“ gemini-3-flash-preview\n",
      "ğŸ“ gemini-3-pro-image-preview\n",
      "ğŸ“ nano-banana-pro-preview\n",
      "ğŸ“ gemini-robotics-er-1.5-preview\n",
      "ğŸ“ gemini-2.5-computer-use-preview-10-2025\n",
      "ğŸ“ deep-research-pro-preview-12-2025\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ’¡ YukarÄ±daki listeden bir ismi seÃ§ip kodunda kullanabilirsin.\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import json\n",
    "\n",
    "API_KEY = \"AIzaSyD1ploqp5fyAYoGbUI7fpP-_cbhrwCOqIw\"\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"generativelanguage.googleapis.com\")\n",
    "print(\"ğŸ” KullanabileceÄŸin modeller listeleniyor...\\n\")\n",
    "\n",
    "try:\n",
    "    # Google'dan model listesini isteyen Ã¶zel adres\n",
    "    conn.request(\"GET\", f\"/v1beta/models?key={API_KEY}\")\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode(\"utf-8\")\n",
    "    models = json.loads(data)\n",
    "\n",
    "    if res.status == 200:\n",
    "        print(\"âœ… ERIÅEBÄ°LDÄ°ÄÄ°N MODELLER:\")\n",
    "        print(\"-\" * 40)\n",
    "        for m in models.get('models', []):\n",
    "            # Sadece iÃ§erik Ã¼retebilenleri (chat modellerini) gÃ¶ster\n",
    "            if \"generateContent\" in m.get('supportedGenerationMethods', []):\n",
    "                # 'models/' kÄ±smÄ±nÄ± temizleyip sadece ismi yazdÄ±ralÄ±m\n",
    "                clean_name = m['name'].replace('models/', '')\n",
    "                print(f\"ğŸ“ {clean_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"\\nğŸ’¡ YukarÄ±daki listeden bir ismi seÃ§ip kodunda kullanabilirsin.\")\n",
    "    else:\n",
    "        print(f\"âŒ Hata: {res.status}\")\n",
    "        print(f\"Mesaj: {models.get('error', {}).get('message')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ BaÄŸlantÄ± hatasÄ±: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
